<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Session 8: Word embedding</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Overview
  </a>
</li>
<li>
  <a href="schedule.html">
    <span class="fa fa-info"></span>
     
    Schedule
  </a>
</li>
<li>
  <a href="1_RBasics.html">
    <span class="fa fa-book"></span>
     
    Session 1
  </a>
</li>
<li>
  <a href="2_ScrapingAndReadingInDocuments.html">
    <span class="fa fa-book"></span>
     
    Session 2
  </a>
</li>
<li>
  <a href="3_CleaningAndManipulating.html">
    <span class="fa fa-book"></span>
     
    Session 3
  </a>
</li>
<li>
  <a href="4_WordFreq.html">
    <span class="fa fa-book"></span>
     
    Session 4
  </a>
</li>
<li>
  <a href="5_SentAndDict.html">
    <span class="fa fa-book"></span>
     
    Session 5
  </a>
</li>
<li>
  <a href="6_DocSimil.html">
    <span class="fa fa-book"></span>
     
    Session 6
  </a>
</li>
<li>
  <a href="7_topicmodelling.html">
    <span class="fa fa-book"></span>
     
    Session 7
  </a>
</li>
<li>
  <a href="8_WordEmbedding.html">
    <span class="fa fa-book"></span>
     
    Session 8
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Session 8: Word embedding</h1>

</div>


<div id="model-training" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Model training</h1>
<p>This tutorial follows the vignettes written by Ben Schmidt to illustrate his <code>wordVectors</code> package. See the <a href="https://github.com/bmschmidt/wordVectors/blob/master/vignettes/introduction.Rmd">introductary</a> and <a href="https://github.com/bmschmidt/wordVectors/blob/master/vignettes/exploration.Rmd">exploration</a> vignettes. See, too, his longer blog <a href="http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">post</a> on vector space models for the humanities</p>
<p>This tutorial walks through training the model on our Nobel corpus which is a <em>very</em> small corpus for word embedding. This is done more as a demonstration of the <code>wordVectors</code> package than as something likely to give us valuable insight. First we write our Nobel corpus to a text file and save it to our current directory, then we instruct <code>wordVectors</code> to prep this file by tokenizing it, changing all upper cases to lower (we’ve already done this but the package doesn’t know that) and looking for commonly occurring bigrams. Then we train the model, which requires it write to another file in so doing, which we duly indicate.</p>
<pre class="r"><code>library(wordVectors)
library(magrittr)
library(tidyverse)
nobel &lt;- read_rds(&quot;data/nobel_cleaned.Rds&quot;)
write_lines(nobel$AwardSpeech, &quot;nobel.txt&quot;)
prep_word2vec(origin=&quot;nobel.txt&quot;,destination=&quot;nobel_prep.txt&quot;,lowercase=T,bundle_ngrams=2)
model &lt;- train_word2vec(&quot;nobel_prep.txt&quot;,&quot;nobel_vectors.bin&quot;, vectors = 200, threads = 4 , window = 10, iter = 5, negative_samples = 10, force = TRUE)</code></pre>
</div>
<div id="vector-operations" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Vector operations</h1>
<p>With our model trained, the most obvious thing to do is to look at individual words and see which other words are closest to them in terms of cosine similarity.</p>
<pre class="r"><code>model %&gt;% closest_to(&quot;peace&quot;, n = 15)</code></pre>
<pre><code>##             word similarity to &quot;peace&quot;
## 1          peace             1.0000000
## 2   prerequisite             0.6288218
## 3  understanding             0.6189000
## 4          noble             0.6155183
## 5     fraternity             0.6109342
## 6     congresses             0.6054665
## 7        secured             0.6038749
## 8      promotion             0.5998970
## 9     intentions             0.5886508
## 10   advancement             0.5875710
## 11     realities             0.5864008
## 12     bjÃ¸rnson             0.5814551
## 13   brotherhood             0.5744863
## 14      symbolic             0.5661090
## 15          jody             0.5620657</code></pre>
<p><code>closest_to</code> allows for easy vector addition and subtraction. We can, for example, try the classic (and perhaps a bit tired) example:</p>
<pre class="r"><code>model %&gt;% closest_to(~&quot;king&quot;+&quot;woman&quot;-&quot;man&quot;)</code></pre>
<pre><code>## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length
## &gt; 1 and only the first element will be used</code></pre>
<pre><code>##               word similarity to &quot;king&quot; + &quot;woman&quot; - &quot;man&quot;
## 1             king                              0.8065347
## 2    martin_luther                              0.6981076
## 3   nelson_mandela                              0.6646871
## 4  andrei_sakharov                              0.6627515
## 5           carlos                              0.6426195
## 6          clinton                              0.5954598
## 7            woman                              0.5869318
## 8          company                              0.5792087
## 9             1983                              0.5776339
## 10  mahatma_gandhi                              0.5771292</code></pre>
<p>Well that didn’t work! But we shouldn’t really be surprised, we’re using a tiny corpus and one not likely to be talking too much about kings or queens. More meaningful for this sort of corpus might be:</p>
<pre class="r"><code>model %&gt;% closest_to(~&quot;nuclear&quot; + &quot;peace&quot;)</code></pre>
<pre><code>## Warning in if (class(object) == &quot;VectorSpaceModel&quot;) {: the condition has length
## &gt; 1 and only the first element will be used</code></pre>
<pre><code>##                        word similarity to &quot;nuclear&quot; + &quot;peace&quot;
## 1                   nuclear                         0.8540356
## 2                     peace                         0.7461462
## 3                  test_ban                         0.7265795
## 4                preventing                         0.7044267
## 5                   testing                         0.6952471
## 6  international_physicians                         0.6932052
## 7                  criminal                         0.6845064
## 8                prevention                         0.6788094
## 9                    ican&#39;s                         0.6785229
## 10               explosions                         0.6710658</code></pre>
<pre class="r"><code>model %&gt;% closest_to(~&quot;nuclear&quot; - &quot;peace&quot;)</code></pre>
<pre><code>##               word similarity to &quot;nuclear&quot; - &quot;peace&quot;
## 1          nuclear                         0.7187029
## 2           atomic                         0.4406238
## 3             bomb                         0.4258041
## 4  nuclear_weapons                         0.4162137
## 5       explosions                         0.4070964
## 6            bombs                         0.3953769
## 7             test                         0.3878737
## 8          testing                         0.3784207
## 9         hydrogen                         0.3696047
## 10         warfare                         0.3645701</code></pre>
<p>A rough approximation of <span class="citation">Kozlowski, Taddy, and Evans (2019)</span> might be to construct a “cultural” vector (we’ll just use one binary pair and take the difference rather than averaging over multiple pairs) and then measuring cosine similarity to other words – ie to what extent they point in the direct of our vector (in the direction of “peace”) or towards “violence” (which will be a negative number, the lower the more similar).</p>
<pre class="r"><code>peace &lt;- model[rownames(model) == &quot;peace&quot;]
violence &lt;- model[rownames(model) == &quot;violence&quot;]
pv_spectrum &lt;- peace-violence
cosineSimilarity(pv_spectrum, model[[&quot;treaty&quot;]])</code></pre>
<pre><code>##           [,1]
## [1,] 0.2675029</code></pre>
<pre class="r"><code>cosineSimilarity(pv_spectrum, model[[&quot;armistice&quot;]])</code></pre>
<pre><code>##            [,1]
## [1,] 0.04961305</code></pre>
<pre class="r"><code>cosineSimilarity(pv_spectrum, model[[&quot;violation&quot;]])</code></pre>
<pre><code>##            [,1]
## [1,] -0.1467678</code></pre>
<pre class="r"><code>cosineSimilarity(pv_spectrum, model[[&quot;aggression&quot;]])</code></pre>
<pre><code>##            [,1]
## [1,] -0.3415268</code></pre>
<pre class="r"><code>cosineSimilarity(pv_spectrum, model[[&quot;war&quot;]])</code></pre>
<pre><code>##            [,1]
## [1,] -0.1495203</code></pre>
<p>All told and for such a small corpus, this seems not half bad.</p>
</div>
<div id="plotting" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Plotting</h1>
<p>We might also try to plot this cultural axis using two binary opposite word pairs and then see where other words land in similarity to the difference between the binaries (similar to what we did above). In order to subset our total corpus, we’ll plot the 200 words most similar to “politics.”</p>
<pre class="r"><code>violation &lt;- model %&gt;% 
  closest_to(~ &quot;violation&quot;-&quot;rights&quot;,n=Inf) 
war &lt;- model %&gt;% 
  closest_to(~ &quot;war&quot; - &quot;peace&quot;, n=Inf)
politics &lt;- model %&gt;%
  closest_to(&quot;politics&quot;, n = 200)

library(ggplot2)
library(dplyr)
politics %&gt;%
  inner_join(violation) %&gt;%
  inner_join(war) %&gt;%
  ggplot() + 
  geom_text(aes(x=`similarity to &quot;violation&quot; - &quot;rights&quot;`,
                y=`similarity to &quot;war&quot; - &quot;peace&quot;`,
                label=word))</code></pre>
<p><img src="8_WordEmbedding_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p><code>wordVectors</code> includes multiple nice plotting features. One is via principle component analysis, which reduces many dimensions to a smaller number (here 2) based on the two most informative dimensions running through the original many-dimensional space. Here we’ll try to compare words grouped around “peace.”</p>
<pre class="r"><code>peacewords &lt;- model %&gt;% closest_to(&quot;peace&quot;, n = 50)
peace = model[[peacewords$word,average=F]]
plot(peace,method=&quot;pca&quot;)</code></pre>
<p><img src="8_WordEmbedding_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Or we can use t-sne, another dimension reduction method to project our word vectors onto two-dimensional space.</p>
<pre class="r"><code>plot(model,perplexity=50)</code></pre>
<p><img src="8_WordEmbedding_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This definitely shows words that tend to show up together. Perhaps some interesting things here, though historians are probably likely to find graphs like this most interesting compared over time.</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-kozlowski2019geometry" class="csl-entry">
Kozlowski, Austin C, Matt Taddy, and James A Evans. 2019. <span>“The Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.”</span> <em>American Sociological Review</em> 84 (5): 905–49.
</div>
</div>
</div>

<br><br><p>2021. <a href="mailto: gregory.fergusoncradler@inn.no">gregory.fergusoncradler@inn.no</a></p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
